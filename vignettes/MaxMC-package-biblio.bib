@article{ardia_differential_2011,
abstract = {The R package DEoptim implements the Differential Evolution algorithm. This al-gorithm is an evolutionary technique similar to classic genetic algorithms that is useful for the solution of global optimization problems. In this note we provide an introduction to the package and demonstrate its utility for financial appli-cations by solving a non-convex portfolio opti-mization problem.},
author = {Ardia, David and Boudt, Kris and Carl, Peter and Mullen, Katharine M and Peterson, Brian G},
issn = {20734859},
journal = {The R Journal},
number = {1},
pages = {27--34},
title = {{Differential Evolution with DEoptim An Application to Non-Convex Portfolio Optimiza- tion}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.365.5865{\&}rep=rep1{\&}type=pdf},
volume = {3},
year = {2011}
}
@article{boudjellaba_testing_1992,
abstract = {In the analysis of economic time series, a question often raised is whether a vector of variables causes another one in the sense of Granger. Most of the literature on this topic is concerned with bivariate relationships or uses finite-order autoregressive specifications. The purpose of this article is to develop a causality analysis in the sense of Granger for general vector autoregressive moving average (ARMA) models. We give a definition of Granger noncausality between vectors, which is a natural and simple extension of the notion of Granger noncausality between two variables. In our context, this definition is shown to be equivalent to a more complex definition proposed by Tjostheim. For the class of linear invertible processes, we derive a necessary and sufficient condition for noncausality between two vectors of variables when the latter do not necessarily include all the variables considered in the analysis. This result is then specialized to the class of stationary invertible ARMA processes. Further, relatively simple necessary and sufficient conditions are obtained for two important cases: (1) the case where the two vectors reduce to two variables inside a larger vector including other variables; and (2) the case where the two vectors embody all the variables considered. Test procedures for these necessary and sufficient conditions are discussed. Among other things, it is noted that the necessary and sufficient conditions for noncausality may involve singularities at which standard asymptotic regularity conditions do not hold. To deal with such situations, we propose a sequential approach that leads to bounds tests. Finally, the tests suggested are applied to Canadian money and income data. The tests are based on bivariate and trivariate models of changes in nominal income and two money stocks (M1 and M2). In contrast with the evidence based on bivariate models, we find from the trivariate model that money causes income unidirectionally.},
author = {Boudjellaba, Hafida and Dufour, Jean Marie and Roy, Roch},
doi = {10.1080/01621459.1992.10476263},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Bounds test,Causality test,Granger causality,Invertible linear process,Sequential procedure},
number = {420},
pages = {1082--1090},
title = {{Testing causality between two vectors in multivariate autoregressive moving average models}},
url = {http://www.jstor.org/stable/2290645},
volume = {87},
year = {1992}
}
@book{burns_burstmisc:_2016,
author = {Burns, Pat},
title = {{BurStMisc: Burns Statistics Miscellaneous}},
url = {https://cran.r-project.org/package=BurStMisc},
year = {2016}
}
@book{ghalanos_parma:_2015,
author = {Ghalanos, A},
title = {{parma: Portfolio Allocation and Risk Management Applications.}},
year = {2012}
}
@article{satman_machine_2013,
author = {Satman, Mehmet Hakan},
issn = {13039709},
journal = {Gazi University Journal of Science},
keywords = {Chromosome encoding,Genetic algorithms,Real parameter optimization},
number = {1},
pages = {85--95},
title = {{Machine coded genetic algorithms for real parameter optimization problems}},
url = {http://gujs.gazi.edu.tr/article/view/1060000982},
volume = {26},
year = {2013}
}
@article{smirnov_table_1948,
abstract = {The use of transformations to stabilize the variance of binomial or Poisson data is familiar(Anscombe 1, Bartlett 2, 3, Curtiss 4, Eisenhart 5). The comparison of transformed binomial or Poisson data with percentage points of the normal distribution to make approximate significance tests or to set approximate confidence intervals is less familiar. Mosteller and Tukey 6 have recently made a graphical application of a transformation related to the square-root transformation for such purposes, where the use of "binomial probability paper" avoids all computation. We report here on an empirical study of a number of approximations, some intended for significance and confidence work and others for variance stabilization. For significance testing and the setting of confidence limits, we should like to use the normal deviate K exceeded with the same probability as the number of successes x from n in a binomial distribution with expectation np, which is defined by frac12pi int{\^{}}K-infty e{\^{}}-frac12t{\^{}}2 dt = operatornameProb x leq k mid operatornamebinomial, n, p. The most useful approximations to K that we can propose here are N (very simple), N{\^{}}+ (accurate near the usual percentage points), and N{\^{}}astast (quite accurate generally), where N = 2 (sqrt(k + 1)q - sqrt(n - k)p). (This is the approximation used with binomial probability paper.) N{\^{}}+ = N + fracN + 2p - 112sqrtE,quad E = textlesser of np textand nq, N{\^{}}ast = N + frac(N - 2)(N + 2)12 big(frac1sqrtnp + 1 - frac1sqrtnq + 1big), N{\^{}}astast = N{\^{}}ast + fracN{\^{}}ast + 2p - 112 sqrtEcdotquad E = textlesser of np textand nq. For variance stabilization, the averaged angular transformation sin{\^{}}-1sqrtfracxn + 1 + sin{\^{}}-1 sqrtfracx + 1n+1 has variance within pm 6{\%} of frac1n + frac12 text(angles in radians), frac821n + frac12 text(angles in degrees), for almost all cases where np geq 1. In the Poisson case, this simplifies to using sqrtx + sqrtx + 1 as having variance 1.},
author = {Smirnov, N.},
doi = {10.1214/aoms/1177730256},
isbn = {0121202011},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {2},
pages = {279--281},
title = {{Table for Estimating the Goodness of Fit of Empirical Distributions}},
url = {http://projecteuclid.org/euclid.aoms/1177730256},
volume = {19},
year = {1948}
}
@article{ardia_jump-diffusion_2011,
author = {Ardia, David and David, Juan and Arango, Ospina and G{\'{o}}mez, Norman Diego Giraldo},
doi = {10.5897/JAERD12.088},
isbn = {1202522173},
issn = {03014215},
journal = {Wilmott},
number = {55},
pages = {76--79},
pmid = {910},
title = {{Jump-Diffusion Calibration Using Differential Evolution}},
url = {http://www.wilmott.com/},
volume = {2011},
year = {2011}
}
@book{ardia_deoptim:_2015,
author = {Ardia, David and Mullen, Katharine M and Peterson, Brian G and Ulrich, Joshua},
title = {{DEoptim : Differential Evolution in R}},
url = {http://cran.r-project.org/package=DEoptim},
year = {2004}
}
@misc{banerjee_co-integration_1993,
abstract = {This book provides a wide-ranging account of the literature on co-integration and the modelling of integrated processes (those which accumulate the effects of past shocks). Data series which display integrated behaviour are common in economics, although techniques appropriate to analysing such data are of recent origin and there are few existing expositions of the literature. This book focuses on the exploration of relationships among integrated data series and the exploitation of these relationships in dynamic econometric modelling. The concepts of co-integration and error-correction models are fundamental components of the modelling strategy. This area of time-series econometrics has grown in importance over the past decade and is of interest to econometric theorists and applied econometricians alike. By explaining the important concepts informally, but also presenting them formally, the book bridges the gap between purely descriptive and purely theoretical accounts of the literature. The asymptotic theory of integrated processes is described and the tools provided by this theory are used to develop the distributions of estimators and test statistics. Practical modelling advice, and the use of techniques for systems estimation, are also emphasized. A knowledge of econometrics, statistics, and matrix algebra at the level of a final-year undergraduate or first-year undergraduate course in econometrics is sufficient for most of the book. Other mathematical tools are described as they occur},
author = {Banerjee, Anindya and Dolado, Juan J. and Galbraith, John W. and Hendry, David},
doi = {10.1093/0198288107.001.0001},
institution = {Oxford University Press},
isbn = {9780198288107},
pmid = {1560535},
title = {{Co-integration, Error Correction, and the Econometric Analysis of Non-Stationary Data}},
type = {{\{}OUP{\}} Catalogue},
url = {http://www.oxfordscholarship.com/view/10.1093/0198288107.001.0001/acprof-9780198288107},
year = {1993}
}
@article{barnard_comment_1963,
abstract = {The spectral analysis of stationary point processes in one dimension is developed in some detail as a statistical method of analysis. The asymptotic sampling theory previously established by the author for a class of doubly stochastic Poisson processes is shown to apply also for a class of clustering processes, the spectra of which are contrasted with those of renewal processes. The analysis is given for two illustrative examples, one an artificial Poisson process, the other of some traffic data. In addition to testing the fit of a clustering model to the latter example, the analysis of these two examples is used where possible to check the validity of the sampling theory.},
author = {Barnard, G A},
issn = {0035-9246},
number = {2},
pages = {294},
title = {{Comment on: "The Spectral Analysis of Point Processes" by M.S. Bartlett}},
volume = {25}
}
@article{david_a._dickey_determining_1987,
abstract = {One way of handling nonstationarity in time series is to compute first differences and fit a model to the differenced series unless the differenced series also looks nonstationary. In that case, second- or higher-order differencing is done. To decide if the current degree of differencing is sufficient, one can look at the autocorrelation function for slow decay. A formal statistical test for the need to difference further is available if one is willing to assume that at most one more difference will render the series stationary. In this article, we present a proper sequence of statistical tests that allows the practitioner to handle cases in which a high order of differencing may be needed. The proper sequence is not the traditional sequence, which begins with a test for a single unit root.},
author = {Dickey, David A. and Pantula, Sastry G.},
doi = {10.1080/07350015.1987.10509614},
isbn = {07350015},
issn = {15372707},
journal = {Journal of Business and Economic Statistics},
keywords = {Nonstationarity,Unit root tests},
number = {4},
pages = {455--461},
title = {{Determining the order of differencing in autoregressive processes}},
url = {http://www.jstor.org/stable/1391997},
volume = {5},
year = {1987}
}
@article{dufour_exact_1996,
abstract = {Several finite-sample tests of parameter constancy against the presence of structural change are proposed for a linear regression model with one lagged dependent variable and independent normal disturbances. The procedures derived include analysis-of-covariance, CUSUM, CUSUM-of-squares, and predictive tests. The approach used to obtain the tests involves the application of three techniques: derivation of an exact confidence set for the autoregressive parameter (based on using an appropriately extended regression), a union-intersection technique, and (when required) randomization. The tests proposed are illustrated with some artificial data and applied to a dynamic trend model of gross private domestic investment in the U.S.},
author = {Dufour, Jean Marie and Kiviet, Jan F.},
doi = {10.1016/0304-4076(94)01683-6},
isbn = {0304407694016},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Exact inference,Finite-sample tests,First-order autoregressive model,Randomization,Structural change},
number = {1},
pages = {39--68},
title = {{Exact tests for structural change in first-order dynamic models}},
url = {http://econpapers.repec.org/article/eeeeconom/v{\_}3a70{\_}3ay{\_}3a1996{\_}3ai{\_}3a1{\_}3ap{\_}3a39-68.htm},
volume = {70},
year = {1996}
}
@incollection{dufour_monte_2003,
abstract = {This chapter contains section titled:

* {\{}INTRODUCTION{\}}
* {\{}STATISTICAL{\}} {\{}ISSUES{\}}: A {\{}PRACTICAL{\}} {\{}APPROACH{\}} {\{}TO{\}} {\{}CORE{\}} {\{}QUESTIONS{\}}
* {\{}THE{\}} {\{}MONTE{\}} {\{}CARLO{\}} {\{}TEST{\}} {\{}TECHNIQUE{\}}: {\{}AN{\}} {\{}EXACT{\}} {\{}RANDOMIZED{\}} {\{}TEST{\}} {\{}PROCEDURE{\}} * {\{}MONTE{\}} {\{}CARLO{\}} {\{}TESTS{\}}: {\{}ECONOMETRIC{\}} {\{}APPLICATIONS{\}}
* {\{}CONCLUSION{\}}},
author = {Dufour, Jean-marie},
booktitle = {A Companion to Theoretical Econometrics},
editor = {Baltagi, Badi H},
isbn = {978-0-470-99624-9},
keywords = {Monte Carlo test methods,econometrics,multivariate regression models,nuisance parameters,randomized test procedure},
number = {August 1999},
pages = {494--519},
publisher = {Blackwell Publishing Ltd},
title = {{Monte Carlo Test Methods in Econometrics {\pounds} Monte Carlo tests : econometric applications}},
volume = {1},
year = {2001}
}
@inproceedings{eberhart_new_1995,
abstract = {The optimization of nonlinear functions using particle swarm$\backslash$nmethodology is described. Implementations of two paradigms are discussed$\backslash$nand compared, including a recently developed locally oriented paradigm.$\backslash$nBenchmark testing of both paradigms is described, and applications,$\backslash$nincluding neural network training and robot task learning, are proposed.$\backslash$nRelationships between particle swarm optimization and both artificial$\backslash$nlife and evolutionary computation are reviewed},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Eberhart, R. and Kennedy, J.},
booktitle = {MHS'95. Proceedings of the Sixth International Symposium on Micro Machine and Human Science},
doi = {10.1109/MHS.1995.494215},
eprint = {9780201398298},
isbn = {0-7803-2676-8},
issn = {1941-0026},
pages = {39--43},
pmid = {20371407},
publisher = {New York, {\{}NY{\}}},
title = {{A new optimizer using particle swarm theory}},
url = {http://ieeexplore.ieee.org/document/494215/},
volume = {1}
}
@article{fisher_fiducial_1935,
author = {FISHER, R. A.},
doi = {10.1111/j.1469-1809.1935.tb02120.x},
issn = {20501420},
journal = {Annals of Eugenics},
number = {4},
pages = {391--398},
title = {{the Fiducial Argument in Statistical Inference}},
url = {http://doi.wiley.com/10.1111/j.1469-1809.1935.tb02120.x},
volume = {6},
year = {1935}
}
@article{mullen_deoptim:_2011,
abstract = {This article describes the R package  DEoptim , which implements the differential evolution algorithm for global optimization of a real-valued function of a real-valued parameter vector. The implementation of differential evolution in  DEoptim  interfaces with C code for efficiency. The utility of the package is illustrated by case studies in fitting a Parratt model for X-ray reflectometry data and a Markov-switching generalized autoregressive conditional heteroskedasticity model for the returns of the Swiss Market Index.},
author = {Mullen, Katharine and Ardia, David and Gil, David and Windover, Donald and Cline, James},
doi = {10.18637/jss.v040.i06},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {6},
pages = {1--26},
pmid = {289228700001},
title = {{DEoptim : An R Package for Global Optimization by Differential Evolution}},
url = {http://www.jstatsoft.org/v40/i06/},
volume = {40},
year = {2011}
}
@article{welch_significance_1938,
abstract = {E' il t-test quando le varianze non sono uguali. Ovvero quando il test di Levene {\`{e}} significativo.},
author = {Weir, John B.De V.},
doi = {10.1038/187438a0},
isbn = {00063444},
issn = {00280836},
journal = {Nature},
number = {4735},
pages = {438},
title = {{Significance of the difference between two means when the population variances may be unequal}},
volume = {187},
year = {1960}
}
@book{wickham_scales:_2016,
author = {Wickham, Hadley},
booktitle = {R package version 0. 4. 0, URL https://CRAN. R-project. org/package= scales},
title = {{Scales: scale functions for visualization}},
url = {https://cran.r-project.org/package=scales},
year = {2016}
}
@book{wickham_roxygen2:_2015,
author = {Wickham, Hadley and Danenberg, Peter and Eugster, Manuel and RStudio},
title = {{roxygen2: In-Line Documentation for R}},
url = {https://cran.r-project.org/package=roxygen2},
year = {2017}
}
@book{zeileis_dynlm:_2014,
author = {Zeileis, Achim},
title = {{{\{}dynlm{\}}: Dynamic Linear Regression}},
url = {http://cran.r-project.org/package=dynlm},
year = {2010}
}
@article{dwass_modified_1957,
abstract = {Suppose {\$}X{\_}1, \backslashcdots, X{\_}m, Y{\_}1, \backslashcdots, Y{\_}n{\$} are {\$}m + n = N{\$} independent random variables, the {\$}X{\$}'s identically distributed and the {\$}Y{\$}'s identically distributed, each with a continuous cdf. Let {\$}{\$}z = (z{\_}1, $\backslash$cdots, z{\_}m, z{\_}{\{}m + 1{\}}, $\backslash$cdots, z{\_}N) = (x{\_}1, $\backslash$cdots, x{\_}m, y{\_}1, $\backslash$cdots, y{\_}n){\$}{\$} represent an observation on the {\$}N{\$} random variables and let {\$}{\$}u(z) = (1/m) $\backslash$sum{\^{}}m{\_}{\{}i = 1{\}} z{\_}i - (1/n) $\backslash$sum{\^{}}N{\_}{\{}i = m + 1{\}} z{\_}i = $\backslash$bar x - $\backslash$bar y{\$}{\$}. Consider the {\$}r = N! N{\$}-tuples obtained from {\$}(z{\_}1, \backslashcdots, z{\_}N){\$} by making all permutations of the indices {\$}(1, \backslashcdots, N){\$}. Since we assume continuous cdf's, then with probability one, these {\$}r N{\$}-tuples will be distinct. Denote them by {\$}z{\^{}}{\{}(1){\}}, \backslashcdots, z{\^{}}{\{}(r){\}}{\$}, and suppose that they have been ordered so that {\$}{\$}u(z{\^{}}{\{}(1){\}} $\backslash$geqq $\backslash$cdots $\backslash$geqq u(z{\^{}}{\{}(r){\}}){\$}{\$}. Notice that since {\$}{\$}$\backslash$bar x - $\backslash$bar y = (1/m) $\backslash$sum{\^{}}N{\_}{\{}i = 1{\}} z{\_}i - (N/m)$\backslash$bar y = (N/n)$\backslash$bar x - (1/n) $\backslash$sum{\^{}}N{\_}{\{}i = 1{\}} z{\_}i,{\$}{\$} the same ordering can be induced by choosing {\$}u(z) = c\backslashbar x{\$} or {\$}u(z) = - c\backslashbar y{\$} for any {\$}c {\textgreater} 0{\$}. Assuming that the cdf's of {\$}X{\_}1, Y{\_}1{\$} are of the form {\$}F(x), F(x - \backslashDelta){\$} respectively, Pitman [2] suggested essentially the following test of the hypothesis {\$}H'{\$} that {\$}\backslashDelta = 0{\$}. Select a set of {\$}k (k {\textgreater} 0){\$} integers {\$}i{\_}1, \backslashcdots, i{\_}k, (1 \backslashleqq i{\_}1 {\textless} \backslashcdots {\textless} i{\_}k \backslashleqq r){\$}. If the observed {\$}z{\$} is one of the points {\$}z{\^{}}{\{}(i{\_}1){\}}, \backslashcdots, z{\^{}}{\{}(i{\_}k){\}}{\$}, reject {\$}H'{\$}, otherwise accept. When {\$}H'{\$} is true, the type one error does not depend on the specific form of the distribution of the {\$}X{\$}'s and the {\$}Y{\$}'s and is in fact equal to {\$}k/r{\$}. The choice of the rejection set {\$}i{\_}1, \backslashcdots, i{\_}k{\$} should depend on the alternative hypothesis. For instance, if the experimenter wants protection against the alternative that the "{\$}X{\$}'s tend to be larger than the {\$}Y{\$}'s," then the labels {\$}1, \backslashcdots, k{\$} might be reasonable. For the alternative that the "{\$}X{\$}'s tend to be smaller than the {\$}Y{\$}'s" the analogous procedure is to use the other tail, {\$}r - k + 1, \backslashcdots, r{\$}. Against both alternatives, a two-tail procedure could be used. Lehmann and Stein have shown in [1] that in the class of all tests (of size {\$}\backslashalpha = k/r{\$}) of the hypothesis {\$}{\$}H: $\backslash$text{\{}the distribution of{\}} X{\_}1 $\backslash$cdots, X{\_}m, Y{\_}1, $\backslash$cdots, Y{\_}n $\backslash$text{\{}is invariant under all permutations{\}},{\$}{\$} the single-tail test based on {\$}1, \backslashcdots, k{\$} is uniformly most powerful against the alternatives that {\$}F{\_}1{\$} is an {\$}N(\backslashtheta, \backslashsigma){\$} cdf, {\$}F{\_}2{\$} is an {\$}N(\backslashtheta + \backslashDelta, \backslashsigma){\$} cdf, {\$}\backslashDelta {\textless} 0{\$}; the test based on {\$}r - k + 1, \backslashcdots, r{\$} is uniformly most powerful for {\$}\backslashDelta {\textgreater} 0{\$}. A practical shortcoming of this procedure is the great difficulty in enumerating the points {\$}z{\^{}}{\{}(i){\}}{\$} and the evaluation of {\$}u(z{\^{}}{\{}(i){\}}){\$} for each of them. For instance, even after eliminating those permutations which always give the same value of {\$}u{\$}, then for sample sizes {\$}m = n = 5{\$}, there are {\$}\backslashbinom{\{}10{\}}{\{}5{\}} = 252{\$} permutations to examine, and for sample sizes {\$}m = n = 10{\$}, there are {\$}\backslashbinom{\{}20{\}}{\{}10{\}} = 184,765{\$} permutations to examine. In the following section, we propose the almost obvious procedure of examining a "random sample" of permutations and making the decision to accept or reject {\$}H{\$} on the basis of those permutations only. Bounds are determined for the ratio of the power of the original procedure to the modified one. Some numerical values of these bounds are given in Table 1. The bounds there listed correspond to tests which in both original and modified form have size {\$}\backslashalpha{\$}, and for which the modified test is based on a random sample of {\$}s{\$} permutations drawn with replacement. These have been computed for a certain class of alternatives which is described below. For simplicity, we have restricted the main exposition to the two-sample problem. In Section 5, we point out extensions to the more general hypotheses of invariance studied in [1].},
archivePrefix = {arXiv},
arxivId = {arXiv:1306.3979v1},
author = {Dwass, Meyer},
doi = {10.1214/aoms/1177707045},
eprint = {arXiv:1306.3979v1},
isbn = {00034851},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
keywords = {Methods,Monte Carlo Hypothesis Testing},
number = {1},
pages = {181--187},
pmid = {10226},
title = {{Modified Randomization Tests for Nonparametric Hypotheses}},
url = {http://www.jstor.org/stable/2237031},
volume = {28},
year = {1957}
}
@article{fisher_asymptotic_1941,
author = {Fisher, Ronald Aylmer},
doi = {10.1111/j.1469-1809.1941.tb02281.x},
issn = {20501420},
journal = {Annals of Eugenics},
number = {1},
pages = {141--172},
title = {{The Asymptotic Approach to Behren's Integral, with Further Tables for the d Test of Significance}},
url = {http://doi.wiley.com/10.1111/j.1469-1809.1941.tb02281.x},
volume = {11},
year = {1941}
}
@article{kirkpatrick_optimization_1983,
abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
doi = {10.1126/science.220.4598.671},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {00368075},
journal = {Science},
number = {4598},
pages = {671--680},
pmid = {25246403},
title = {{Optimization by simulated annealing}},
volume = {220},
year = {1983}
}
@book{willighagen_genalg:_2015,
author = {Willighagen, E},
booktitle = {R package version 0.1},
title = {{Genalg: R based genetic algorithm}},
url = {https://cran.r-project.org/package=genalg},
volume = {1},
year = {2005}
}
@book{zambrano-bigiarini_hydropso:_2014,
author = {Zambrano-Bigiarini, M},
title = {{Particle Swarm Optimisation, with focus on Environmental Models}},
url = {http://www.rforge.net/hydroPSO,{\%}5Cnhttp://cran.r-project.org/web/packages/hydroPSO},
year = {2013}
}
@book{bendtsen_pso:_2012,
author = {Bendtsen, Claus},
title = {{pso: Particle Swarm Optimization}},
url = {https://cran.r-project.org/package=pso},
year = {2012}
}
@book{ciupke_psoptim:_2016,
abstract = {The particle swarm optimization (PSO) algorithm is a population-based search al-gorithm based on the simulation of the social behavior of birds within a flock. The initial intent of the particle swarm concept was to graphically simulate the graceful and unpredictable choreography of a bird flock [449], with the aim of discovering pat-terns that govern the ability of birds to fly synchronously, and to suddenly change direction with a regrouping in an optimal formation. From this initial objective, the concept evolved into a simple and efficient optimization algorithm. In PSO, individuals, referred to as particles, are " flown " through hyperdimensional search space. Changes to the position of particles within the search space are based on the social-psychological tendency of individuals to emulate the success of other individuals. The changes to a particle within the swarm are therefore influenced by the experience, or knowledge, of its neighbors. The search behavior of a particle is thus affected by that of other particles within the swarm (PSO is therefore a kind of symbiotic cooperative algorithm). The consequence of modeling this social behavior is that the search process is such that particles stochastically return toward previously successful regions in the search space. The remainder of this chapter is organized as follows: An overview of the basic PSO, i.e. the first implementations of PSO, is given in Section 16.1. The very important concepts of social interaction and social networks are discussed in Section 16.2. Basic variations of the PSO are described in Section 16.3, while more elaborate improvements are given in Section 16.5. A discussion of PSO parameters is given in Section 16.4. Some advanced topics are discussed in Section 16.6.},
author = {Ciupke, Krzysztof},
isbn = {9781905209040},
issn = {1941-0492},
title = {{Particle Swarm Optimization}},
url = {https://cran.r-project.org/package=psoptim}
}
@book{davison_bootstrap_1997,
author = {Davison, A C and Hinkley, D V},
publisher = {Cambridge University Press},
title = {{Bootstrap methods and their applications}},
url = {http://statwww.epfl.ch/davison/BMA/},
year = {2009}
}
@article{dufour_wald_2013,
abstract = {Wald-type tests are convenient because they allow one to test a wide array of linear and nonlinear restrictions from a single unrestricted estimator; we focus on the problem of implementing Wald-type tests for nonlinear restrictions. We provide examples showing that Wald statistics in non-regular cases can have several asymptotic distributions; the usual critical values based on a chi-square distribution can both lead to under-rejections and over-rejections; indeed, the Wald statistic may diverge under the null hypothesis. We study the asymptotic distribution of Wald-type statistics for the class of polynomial restrictions and show that the Wald statistic either has a non-degenerate asymptotic distribution, or diverges to infinity. We provide conditions for convergence and a general characterization of this distribution. We provide bounds on the asymptotic distribution (when it exists). In several cases of interest, this bound yields an easily available conservative critical value. We propose an adaptive consistent strategy for determining whether the asymptotic distribution exists and which form it takes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.0569v1},
author = {Dufour, Jean-marie and Renault, Eric and Zinde-walsh, Victoria},
eprint = {arXiv:1312.0569v1},
keywords = {bound,by the willam dow,c3,chair in political economy,deficient rank,journal of eco-,mcgill,nomic literature classification,nonlinear restriction,nonstandard asymptotic theory,singular covariance ma-,this work was supported,trix,wald test},
pages = {1--48},
title = {{Wald tests when restrictions are locally}},
year = {2013}
}
@article{dufour_rank-robust_2016,
author = {Dufour, Jean-Marie and Val{\'{e}}ry, Pascale},
title = {{Rank-robust Wald-type tests: a regularization approach}}
}
@article{haldrup_robustness_2002,
author = {Haldrup, Niels and Lildholdt, Peter},
doi = {10.1111/1467-9892.00260},
issn = {01439782},
journal = {Journal of Time Series Analysis},
keywords = {Dickey-Fuller test,I(1) versus I(2),Phillips-Perron test,Unit root tests},
number = {2},
pages = {155--171},
title = {{On the robustness of unit root tests in the presence of double unit roots}},
volume = {23},
year = {2002}
}
@article{kirkpatrick_optimization_1984,
abstract = {Simulated annealing is a stochastic optimization procedure which is widely applicable and has been found effective in several problems arising in computeraided circuit design. This paper derives the method in the context of traditional optimization heuristics and presents experimental studies of its computational efficiency when applied to graph partitioning and traveling salesman problems.},
author = {Kirkpatrick, Scott},
doi = {10.1007/BF01009452},
isbn = {0022-4715},
issn = {00224715},
journal = {Journal of Statictical Physics},
keywords = {Spin glasses,algorithms,graph partitioning,optimization,spin glasses},
number = {5-6},
pages = {975--986},
pmid = {17813860},
title = {{Optimization by simulated annealing: Quantitative studies}},
url = {http://link.springer.com/article/10.1007/BF01009452},
volume = {34},
year = {1984}
}
@book{r_core_team_r:_2016,
archivePrefix = {arXiv},
arxivId = {http://www.R-project.org},
author = {null Null},
doi = {10.1038/sj.hdy.6800737},
eprint = {/www.R-project.org},
isbn = {3-900051-07-0},
issn = {16000706},
pmid = {16106260},
primaryClass = {http:},
publisher = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2017}
}
@book{trautmann_cmaes:_2011,
author = {Trautmann, Heike and Mersmann, Olaf and Arnu, David},
title = {{cmaes: Covariance Matrix Adapting Evolutionary Strategy}},
url = {https://cran.r-project.org/package=cmaes}
}
@book{wickham_devtools:_2016,
abstract = {Collection of package development tools.},
author = {Wickham, Hadley and Chang, Winston and RStudio and namespace and vignette code extracted from Base$\backslash$nR), R. Core team (Some},
title = {{devtools: Tools to make developing R code easier}},
url = {http://cran.r-project.org/web/packages/devtools/index.html},
year = {2014}
}
@article{zambrano-bigiarini_model-independent_2013,
author = {Zambrano-Bigiarini and M. and Rojas and R.},
journal = {Environmental Modelling {\&} Software},
pages = {5--25},
title = {{A model-independent Particle Swarm Optimisation software for model calibration}},
url = {http://dx.doi.org/10.1016/j.envsoft.2013.01.004},
volume = {43},
year = {2013}
}
@article{behrens_beitrag_1929,
author = {Behrens, W V},
journal = {Landwirtsch Jahrbucher},
pages = {807--837},
title = {{Ein beitrag zur fehlerberechnung bei wenigen beobachtungen}},
volume = {68},
year = {1929}
}
@misc{boudjellaba_simplified_1992,
abstract = {No abstract is available for this item.},
author = {Boudjellaba, H and Dufour, J.{\~{}}M. and Roy, R},
institution = {Universite de Montreal, Departement de sciences economiques},
keywords = {econometrics,{\{}ECONOMIC{\}} {\{}MODELS{\}}},
number = {9236},
title = {{Conditions for non-causality between two subvectors in multivariate {\{}ARMA{\}} models}},
type = {Cahiers de recherche},
url = {https://ideas.repec.org/p/mtl/montde/9236.html},
year = {1991}
}
@book{clayden_soma:_2014,
abstract = {This package provides an R implementation of the Self-Organising Migrating Algorithm, a general-purpose, stochastic optimisation algorithm. The approach is similar to that of genetic algorithms, although it is based on the idea of a series of ``migrations'' by a fixed set of individuals, rather than the development of successive generations. It can be applied to any costminimisation problem with a bounded parameter space, and is robust to local minima.},
author = {Clayden, J.},
title = {{General-Purpose Optimisation With the Self-Organising Migrating Algorithm}},
url = {https://cran.r-project.org/package=soma},
year = {2014}
}
@article{scrucca_ga:_2013,
abstract = {Genetic algorithms (GAs) are stochastic search algorithms inspired by the basic principles of biological evolution and natural selection. GAs simulate the evolution of living organisms, where the fittest individuals dominate over the weaker ones, by mimicking the biological mechanisms of evolution, such as selection, crossover and mutation. GAs have been successfully applied to solve optimization problems, both for continuous (whether differentiable or not) and discrete functions. This paper describes the R package GA , a collection of general purpose functions that provide a flexible set of tools for applying a wide range of genetic algorithm methods. Several examples are discussed, ranging from mathematical functions in one and two dimensions known to be hard to optimize with standard derivative-based methods, to some selected statistical problems which require the optimization of user defined objective functions. (This paper contains animations that can be viewed using the Adobe Acrobat PDF viewer.)},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.0228},
author = {Scrucca, Luca},
doi = {10.18637/jss.v053.i04},
eprint = {arXiv:1501.0228},
isbn = {1548-7660},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {4},
pages = {1--37},
pmid = {18291371},
title = {{GA : A Package for Genetic Algorithms in R}},
url = {http://www.jstatsoft.org/v53/i04/},
volume = {53},
year = {2013}
}
@article{pantula_testing_1989,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Pantula, G. Sastry},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Econometric Theory},
keywords = {icle},
number = {2},
pages = {256--271},
pmid = {25246403},
title = {{Testing for Unit Roots in Time Series Data}},
url = {http://www.jstor.org/stable/3532498},
volume = {5},
year = {1989}
}
@book{geyer_mcmc:_2015,
abstract = {Markov chain Monte Carlo (MCMC) is a powerful technique for performing integration by simulation. In recent years, MCMC has revolutionized the appli-cation of Bayesian statistics. Many high-dimensional, complex models which were formerly intractable can now be handled routinely. MCMC has also been used in specialized non-Bayesian problems. Introductory material on MCMC methods and biostatistical appli-cations can be found in Gilks et al. [20] and Gelman {\&} Rubin [13]. Suppose that we wish to evaluate the expected value (expectation) of some function g($\theta$) over a probability density function f ($\theta$) : E f [g($\theta$)] = g($\theta$)f ($\theta$) d$\theta$. If we could draw samples $\theta$ (1) , $\theta$ (2) , . . . , $\theta$ (n) independently from f ($\theta$), then we could estimat{\^{e}} E f [g($\theta$)] =},
archivePrefix = {arXiv},
arxivId = {1001.2058},
author = {Brewer, Brendon J},
doi = {10.1016/B978-0-444-51575-9.00006-3},
eprint = {1001.2058},
isbn = {9780444515759},
issn = {0037-7961},
number = {Mcmc},
pages = {1--6},
pmid = {9132895},
title = {{Markov Chain Monte Carlo Markov Chain Monte Carlo}},
url = {https://cran.r-project.org/package=mcmc},
year = {2011}
}
@book{mersmann_microbenchmark:_2015,
abstract = {Provides infrastructure to accurately measure and compare the execution time of R expressions.},
author = {Mersmann, Olaf},
title = {{microbenchmark: Accurate Timing Functions. R Package.}},
url = {https://cran.r-project.org/package=microbenchmark},
year = {2015}
}
@inproceedings{shi_modified_1998,
author = {Shi, Y. and Eberhart, R.},
booktitle = {Proceedings of the IEEE Congress on Evolutionary Computation},
pages = {69--73},
publisher = {IEEE},
title = {{A Modified Particle Swarm Optimizer}},
year = {1998}
}
@article{smirnoff_sur_1939,
abstract = {Рассматривается применение метода статистической линеаризации к решению некоторых задач динамики систем, подверженных случай­ ным возмущениям и имеющих многомерные нелинейные элементы. 1. Общие уравнения системы и метод их анализа Теоретическое исследование задач динамики систем в целом ряде слу­ чаев приводится к интегрированию обыкновенных стохастических диффе­ ренциальных уравнений, включающих безынерционные многомерные не­ линейные элементы мультипликативного и более сложных типов. К таким задачам относятся, например, исследование точности многоканальных авто­ матических систем при учете перекрестной связи каналов, исследование динамики некоторых нелинейных экстремальных самонастраивающихся систем, находящихся под действием случайных возмущений, исследование случайных колебаний упругих систем при учете нелинейных факторов (восстанавливающей силы, демпфирования и инерционности), а также целый ряд других задач. В достаточно общей форме уравнения, характери­ зующие поведение этих систем, могут быть представлены в виде Xi = фг({\#}1, . . . , Хп) + Zi (* = 1, . . . , ft), (1) где фг — однозначные нелинейные функции своих аргументов, Zi(t) — воз­ мущения, являющиеся случайными функциями времени. Для определения основных вероятностных моментов переменных, ха­ рактеризующих поведение упомянутых систем, возможно применение ме­ тода статистической линеаризации [1, 2], распространенного на много­ мерные безынерционные нелинейности *. При применении этого метода нелинейные функции ф г -заменяются статистическими линеаризованными эквивалентами фг({\#}1, . . . , Х п) Ж фго({\^{}}х,, • • • т х п , k XlXl , . . . ,},
author = {Smirnoff, N.},
doi = {10.18287/0134-2452-2015-39-4-459-461.},
journal = {Rec. Math.},
number = {18},
pages = {3--26},
title = {{Sur les écarts de la courbe de distribution empirique}},
volume = {6},
year = {1939}
}
@book{bergmeir_continuous_2012,
author = {Bergmeir, Christoph and Molina, Daniel and Ben{\'{i}}tez, Jos{\'{e}} M.},
pages = {1--9},
title = {{Continuous Optimization using Memetic Algorithms with Local Search Chains (MA-LS-Chains) in R}},
year = {2015}
}
@book{caffo_exactloglintest:_2013,
author = {Caffo, Brian},
title = {{exactloglintest: Monte Carlo Exact Tests for Log-linear models}},
url = {https://cran.r-project.org/package=exactLoglinTest}
}
@book{price_differential_2006,
abstract = {Problems demanding globally optimal solutions are ubiquitous, yet many are intractable when they involve constrained functions having many local optima and interacting, mixed-type variables.The differential evolution (DE) algorithm is a practical approach to global numerical optimization which is easy to understand, simple to implement, reliable, and fast. Packed with illustrations, computer code, new insights, and practical advice, this volume explores DE in both principle and practice. It is a valuable resource for professionals needing a proven optimizer and for students wanting an evolutionary perspective on global numerical optimization. A companion CD includes DE-based optimization software in several programming languages.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Price, Kenneth V and Storn, Rainer M and Lampinen, Jouni A},
booktitle = {New York},
doi = {10.1007/3-540-31306-0},
eprint = {9809069v1},
isbn = {978-3-540-31306-9},
issn = {09255001},
pages = {538},
pmid = {20924084},
primaryClass = {arXiv:gr-qc},
publisher = {Springer-Verlag},
series = {Natural Computing},
title = {{Differential Evolution: A Practical Approach to Global Optimization}},
url = {http://www.springer.com/mathematics/book/978-3-540-20950-8?detailsPage=reviews},
volume = {28},
year = {2005}
}
@article{welch_generalization_1947,
abstract = {statistics},
author = {Welch, B. L.},
doi = {10.2307/2332510},
isbn = {0006-3444 (Print)$\backslash$r0006-3444 (Linking)},
issn = {00063444},
journal = {Biometrika},
number = {1/2},
pages = {28},
pmid = {20287819},
title = {{The Generalization of `Student's' Problem when Several Different Population Variances are Involved}},
url = {http://www.jstor.org/stable/2332510?origin=crossref},
volume = {34},
year = {1947}
}
@book{wuertz_funitroots:_2013,
author = {Wuertz, Diethelm},
title = {{fUnitRoots: Trends and Unit Roots}},
url = {http://cran.r-project.org/package=fUnitRoots},
year = {2009}
}
@article{yang_xiang_generalized_2013,
abstract = {Many problems in statistics, finance, biology, pharmacology, physics, mathematics, economics, and chemistry involve determination of the global minimum of multidimensional functions. R packages for different stochastic methods such as genetic algorithms and differential evolution have been developed and successfully used in the R community. Based on Tsallis statistics, the R package GenSA was developed for generalized simulated annealing to process complicated non-linear objective functions with a large number of local minima. In this paper we provide a brief introduction to the R package and demonstrate its utility by solving a non-convex portfolio optimization problem in finance and the Thomson problem in physics. GenSA is useful and can serve as a complementary tool to, rather than a replacement for, other widely used R packages for optimization.},
author = {Xiang, Y and Gubian, Sylvain and Suomela, Brian and Hoeng, Julia},
isbn = {2073-4859},
issn = {20734859},
journal = {R Journal},
number = {June},
pages = {13--28},
title = {{Generalized simulated annealing for global optimization: the GenSA Package}},
url = {http://rjournal.github.io/archive/2013-1/xiang-gubian-suomela-etal.pdf},
volume = {5},
year = {2013}
}
@article{dufour_exact_2001,
abstract = {In this paper, we study several tests for the equality of two unknown distributions. Two are based on empirical distribution functions, three others on nonparametric probability density estimates, and the last ones on differences between sample moments. We suggest ... $\backslash$n},
author = {Dufour, J M and Farhat, A},
title = {{Exact Nonparametric Two-Sample Homogeneity Tests for Possibly Discrete Distributions.}},
url = {https://papyrus.bib.umontreal.ca/xmlui/handle/1866/362{\%}5Cnpapers3://publication/uuid/C9679308-BA5E-4889-BC4D-BACE5A04D136},
year = {2001}
}
@article{dufour_monte_2006,
abstract = {The technique of Monte Carlo (MC) tests [Dwass (1957, Annals of Mathematical Statistics 28, 181-187); Barnard (1963, Journal of the Royal Statistical Society, Series B 25, 294)] provides a simple method for building exact tests from statistics whose finite sample distribution is intractable but can be simulated (when no nuisance parameter is involved). We extend this method in two ways: first, by allowing for MC tests based on exchangeable possibly discrete test statistics; second, by generalizing it to statistics whose null distribution involves nuisance parameters [maximized MC (MMC) tests]. Simplified asymptotically justified versions of the MMC method are also proposed: these provide a simple way of improving standard asymptotics and dealing with nonstandard asymptotics. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Dufour, Jean Marie},
doi = {10.1016/j.jeconom.2005.06.007},
isbn = {03044076},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Asymptotics,Bootstrap,Bounds,Exact test,Finite-sample test,Maximized Monte Carlo test,Monte Carlo test,Nonstandard asymptotic distribution,Nuisance parameter,Parametric bootstrap,Simulated annealing},
number = {2},
pages = {443--477},
shorttitle = {Monte Carlo tests with nuisance parameters},
title = {{Monte Carlo tests with nuisance parameters: A general approach to finite-sample inference and nonstandard asymptotics}},
volume = {133},
year = {2006}
}
@article{fernandez-i-marin_ggmcmc:_2016,
abstract = {ggmcmc is an R package for analyzing Markov chain Monte Carlo simulations from Bayesian inference. By using a well known example of hierarchical/multilevel modeling, the article reviews the potential uses and options of the package, ranging from classical convergence tests to caterpillar plots or posterior predictive checks.},
author = {Fern{\'{a}}ndez-i-Mar{\'{i}}n, Xavier},
doi = {10.18637/jss.v070.i09},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {9},
pages = {1--20},
title = {{ggmcmc : Analysis of MCMC Samples and Bayesian Inference}},
url = {http://www.jstatsoft.org/v70/i09/},
volume = {70},
year = {2016}
}
@book{gilli_numerical_2011,
author = {Gilli, M. and Maringe, D. and Schumann, E.},
publisher = {Academic Press},
title = {{Numerical Methods and Optimization in Finance}},
url = {http://nmof.net},
year = {2011}
}
